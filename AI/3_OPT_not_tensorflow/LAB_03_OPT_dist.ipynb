{"cells":[{"cell_type":"markdown","metadata":{},"source":["AI Programming - SW Lee"]},{"cell_type":"markdown","metadata":{},"source":["# Lab 03: Optimizers for Deep Neural Networks\n","## Exercise: Predicting MNIST Digits\n","### For this exercise, prepare privious Lab to copy your previous implementations."]},{"cell_type":"markdown","metadata":{"id":"NhC9a_u5Ta4u"},"source":["### Prepare Mini-MNIST Dataset"]},{"cell_type":"code","execution_count":182,"metadata":{},"outputs":[],"source":["import numpy as np # 행렬 연산 라이브러리\n","from sklearn.datasets import load_digits # 손글씨 데이터셋\n","from sklearn.model_selection import train_test_split # 데이터셋 분리\n","from sklearn.preprocessing import StandardScaler # 데이터 전처리\n","import matplotlib.pyplot as plt # 그래프 라이브러리"]},{"cell_type":"code","execution_count":183,"metadata":{"id":"MboBxtwcTa41"},"outputs":[],"source":["digits = load_digits() # 손글씨 데이터셋 로드\n","\n","# digits.data from sklearn contains 1797 images of 8x8 pixels\n","# Each image has a hand-written digit\n","digits_df = digits.images.reshape((len(digits.target), -1)) # 8x8 픽셀 이미지를 64개의 픽셀로 변환\n","digits_tf = digits.target # 손글씨 데이터셋의 정답\n","\n","# Splitting dataframe into train & test\n","# 80% train, 20% test\n","X_train_org, X_test_org, y_train_num, y_test = train_test_split(digits_df, digits_tf, test_size= 0.20, random_state= 101)\n","\n","# Digits data has range of [0,16], which often lead too big exponential values\n","# so make them normal distribution of [0,1] with the sklearn package, or you can just divide them by 16\n","\n","# StandarScaler는 각 feature의 평균을 0, 분산을 1로 변경하여 모든 feature들이 같은 scale을 갖도록 만들어줍니다.\n","sc = StandardScaler() \n","X_train = sc.fit_transform(X_train_org) # fit_transform은 fit과 transform을 동시에 수행하는 함수입니다. fit은 데이터의 평균과 분산을 구하는 역할을 합니다.\n","X_test = sc.transform(X_test_org) # transform은 fit에서 설정된 값으로 데이터를 변환합니다.\n","\n","n_classes = 10 # 0~9까지 총 10개의 클래스가 있습니다.\n","\n","# Transform Nx1 Y vector into Nx10 answer vector, so that we can perform one-to-all classification\n","y_train = np.zeros((y_train_num.shape[0],10)) # 0으로 채워진 2차원 배열을 생성합니다.\n","for i in range(n_classes): # 0~9까지 반복합니다.\n","    y_train[:,i] = (y_train_num == i) # y_train_num이 i와 같으면 1로 채웁니다."]},{"cell_type":"markdown","metadata":{},"source":["Define Utility Functions"]},{"cell_type":"code","execution_count":184,"metadata":{},"outputs":[],"source":["from tensorflow.math import sigmoid as tf_sigmoid # tensorflow의 sigmoid 함수를 불러옵니다.\n","from tensorflow.nn import softmax as tf_softmax # tensorflow의 softmax 함수를 불러옵니다.\n","\n","def sigmoid(x): # sigmoid 함수를 정의합니다.\n","    x = tf_sigmoid(x) # tensorflow의 sigmoid 함수를 사용합니다.\n","    return x.numpy() # numpy 배열로 변환하여 반환합니다.\n","\n","def softmax(x): # softmax 함수를 정의합니다.\n","    x = tf_softmax(x) # tensorflow의 softmax 함수를 사용합니다.\n","    return x.numpy() # numpy 배열로 변환하여 반환합니다."]},{"cell_type":"code","execution_count":185,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(1797, 64)\n","(1437, 64)\n","(1437, 10)\n","[ 0.  0.  0.  9. 16.  6.  0.  0.  0.  0.  4. 15.  6. 15.  0.  0.  0.  0.\n","  8. 11.  9. 11.  0.  0.  0.  0.  8. 16. 14.  2.  0.  0.  0.  0. 11. 16.\n"," 13.  0.  0.  0.  0.  6. 14.  2. 12.  9.  0.  0.  0.  5. 16. 11.  5. 13.\n","  4.  0.  0.  0.  3.  8. 13. 16.  9.  0.]\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 200x200 with 1 Axes>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["The number is 3\n"]}],"source":["print(digits_df.shape) # 손글씨 데이터셋의 shape를 출력합니다.\n","print(X_train.shape) # X_train의 shape를 출력합니다.\n","print(y_train.shape) # y_train의 shape를 출력합니다.\n","print(X_train_org[0]) # X_train_org의 첫번째 데이터를 출력합니다.\n","\n","idx = np.random.randint(X_train.shape[0]) # 0부터 X_train의 크기까지 랜덤한 정수를 생성합니다.\n","dimage = X_train_org[idx].reshape((8,8)) # X_train_org의 idx번째 데이터를 8x8로 변환합니다.\n","plt.figure(figsize=(2,2)) # 그래프의 크기를 설정합니다.\n","plt.gray() # 그래프의 색상을 흑백으로 설정합니다.\n","plt.matshow(dimage, fignum=1) # 이미지를 출력합니다.\n","plt.show() # 그래프를 출력합니다.\n","print('The number is', y_train_num[idx]) # idx번째 데이터의 정답을 출력합니다.\n"]},{"cell_type":"markdown","metadata":{},"source":["### Simple DNN for Digit Classification"]},{"cell_type":"markdown","metadata":{},"source":["Define Model Class"]},{"cell_type":"code","execution_count":186,"metadata":{},"outputs":[],"source":["class myDenseLayer:\n","    def __init__(self, n_out, n_in):\n","        self.wegt = np.empty((n_out, n_in)) # n_out x n_in 크기의 빈 행렬을 생성합니다.\n","        self.bias = np.zeros((n_out)) # n_out 크기의 0으로 채워진 행렬을 생성합니다.\n","        self.saved_x = None     # store x to use while backpropagation\n","\n","    def forward(self, x):       # (b, i)\n","        ### START CODE HERE ###\n","        self.saved_x = x     # keep it for backward\n","        x_lin = (self.wegt @ x.T).T + self.bias   # Linear Prediction\n","        ### END CODE HERE ###\n","        return x_lin\n","\n","    def backward(self, x, x_in):  # x = dJ/dz (b, c)\n","        assert np.array_equal(self.saved_x, x_in), print('x_in does not equal to input X.')\n","        ### START CODE HERE ###\n","        \n","        # 가중치에 대한 기울기 계산 (차원에 맞추어 연산)\n","        dw = x.T @ x_in  # @ 연산자를 사용하여 행렬 곱셈 (x.T의 shape과 x_in의 shape에 맞춰 연산)\n","        # 편향에 대한 기울기 계산\n","        db = np.sum(x, axis=0)  # 각 출력에 대한 편향의 기울기\n","        # 이전 레이어로 전달할 기울기 계산\n","        wdJdz = x @ self.wegt  # wegt는 가중치, x는 현재 레이어의 출력, @ 연산자로 행렬 곱셈\n","        ### END CODE HERE ###\n","        return dw, db, wdJdz # return dJ/dw, dJ/db, dJ/dx\n"]},{"cell_type":"markdown","metadata":{},"source":["Define Backpropagation of Activation Functions"]},{"cell_type":"code","execution_count":187,"metadata":{},"outputs":[],"source":["# sigmoid 함수의 기울기를 계산하는 함수를 구현합니다.\n","def dJdz_sigmoid(wdJdz_upper, az):\n","    ### START CODE HERE ###\n","\n","    dJdz = wdJdz_upper * az * (1 - az)            # backpropagation through activation function\n","    \n","    ### END CODE HERE ###\n","    return dJdz # dJ/dz 반환\n","\n","# softmax 함수의 기울기를 계산하는 함수를 구현합니다.\n","def dJdz_softmax(y_hat, y):\n","    ### START CODE HERE ###\n","    \n","    dJdz = y_hat - y            # backpropagation through activation function\n","    \n","    ### END CODE HERE ###\n","    return dJdz # dJ/dz 반환"]},{"cell_type":"markdown","metadata":{},"source":["Define Training Functions"]},{"cell_type":"code","execution_count":188,"metadata":{},"outputs":[],"source":["# 3개의 layer에 대한 forward, backward, loss, predict 함수를 구현합니다.\n","def my_forward(layers, X_in): \n","    # layers = [l1, l2, l3]\n","    l1, l2, l3 = layers\n","    ### START CODE HERE ###\n","\n","    # forward pass\n","    a_1 = sigmoid(l1.forward(X_in))               # first stage forward\n","    a_2 = sigmoid(l2.forward(a_1))                    # second stage forward\n","    a_3 = softmax(l3.forward(a_2))                    # third stage forward\n","\n","    ### END CODE HERE ###\n","    return a_1, a_2, a_3 # return a_1, a_2, a_3\n","\n","# 3개의 layer에 대한 forward, backward, loss, predict 함수를 구현합니다.\n","def my_backward(layers, a_1, a_2, a_3, X_in, y_true):\n","    l1, l2, l3 = layers # layers = [l1, l2, l3]\n","    ### START CODE HERE ###\n","\n","    # backward pass\n","    dw_3, db_3, wdJdz_3 = l3.backward(dJdz_softmax(a_3, y_true) , a_2)    # go through 3rd stage backward\n","    dw_2, db_2, wdJdz_2 = l2.backward(dJdz_sigmoid(wdJdz_3, a_2), a_1)    # go through 2nd stage backward\n","    dw_1, db_1, _       = l1.backward(dJdz_sigmoid(wdJdz_2, a_1)  , X_in)    # go through 1st stage backward\n","\n","    ### END CODE HERE ###\n","    d_1 = [dw_1, db_1] # d_1 = [dJ/dw_1, dJ/db_1]\n","    d_2 = [dw_2, db_2] # d_2 = [dJ/dw_2, dJ/db_2]\n","    d_3 = [dw_3, db_3] # d_3 = [dJ/dw_3, dJ/db_3]\n","    return d_1, d_2, d_3 \n","\n","def my_loss(layers, X_in, y_true):\n","    l1, l2, l3 = layers # layers = [l1, l2, l3]\n","    ### START CODE HERE ###\n","\n","    # forward pass\n","    a_1 = sigmoid(l1.forward(X_in))               # first stage forward\n","    a_2 = sigmoid(l2.forward(a_1))                    # second stage forward\n","    a_3 = softmax(l3.forward(a_2))                    # third stage forward\n","    # calculate loss\n","    # clip 함수를 사용하여 a_3의 값이 0이 되는 것과 1초과하는 것을 방지합니다.\n","    # if np.any((a_3 <= 0) | (a_3 >= 1)):\n","    #     print(\"a_3 out of range: \")\n","    #     print(a_3)\n","    loss = -np.sum(y_true * np.log(np.clip(a_3, 1e-9, 1))) / y_true.shape[0] # y_true와 a_3는 동일한 shape이어야 함         # calculate loss\n","\n","    ### END CODE HERE ###\n","    return loss\n","    \n","def my_predict(layers, X_in):\n","    l1, l2, l3 = layers\n","    ### START CODE HERE ###\n","\n","    # forward pass\n","    a_1 = sigmoid(l1.forward(X_in))               # first stage forward\n","    a_2 = sigmoid(l2.forward(a_1))                    # second stage forward\n","    a_3 = softmax(l3.forward(a_2))                    # third stage forward\n","    \n","    # predict\n","    # a_3의 마지막 차원을 기준으로 가장 큰 값의 인덱스를 반환\n","    pred = np.argmax([a_3], axis=-1).reshape(-1) \n","    ### END CODE HERE ###\n","    return pred"]},{"cell_type":"markdown","metadata":{},"source":["Create a NN model and check the matrix dimensions"]},{"cell_type":"code","execution_count":189,"metadata":{"id":"DT0rMw-rTa5A"},"outputs":[{"name":"stdout","output_type":"stream","text":["(1437, 64) (1437, 10)\n","(80, 64) (80,)\n","(70, 80) (70,)\n","(10, 70) (10,)\n"]}],"source":["n_inputs  = 64  # 8x8 픽셀 이미지를 1차원으로 변환하면 64개의 입력\n","n_hidden1 = 80 # 첫번째 은닉층의 노드 수\n","n_hidden2 = 70 # 두번째 은닉층의 노드 수\n","n_classes = 10 # 0~9까지 총 10개의 클래스\n","\n","l1 = myDenseLayer(n_hidden1, n_inputs) # 첫번째 은닉층\n","l2 = myDenseLayer(n_hidden2, n_hidden1) # 두번째 은닉층\n","l3 = myDenseLayer(n_classes, n_hidden2) # 출력층\n","\n","layers = [l1, l2, l3] # 3개의 layer를 리스트로 저장\n","\n","print(X_train.shape, y_train.shape) # X_train과 y_train의 shape를 출력\n","print(l1.wegt.shape, l1.bias.shape) # l1의 가중치와 편향의 shape를 출력\n","print(l2.wegt.shape, l2.bias.shape) # l2의 가중치와 편향의 shape를 출력\n","print(l3.wegt.shape, l3.bias.shape) # l3의 가중치와 편향의 shape를 출력"]},{"cell_type":"markdown","metadata":{},"source":["Weight Initialization"]},{"cell_type":"code","execution_count":190,"metadata":{},"outputs":[],"source":["# Weights are initialized to...\n","l1.wegt = np.random.randn(n_hidden1, n_inputs) # l1의 가중치를 랜덤한 값으로 초기화합니다.\n","l2.wegt = np.random.randn(n_hidden2, n_hidden1) # l2의 가중치를 랜덤한 값으로 초기화합니다.\n","l3.wegt = np.random.randn(n_classes, n_hidden2) # l3의 가중치를 랜덤한 값으로 초기화합니다."]},{"cell_type":"markdown","metadata":{"id":"aXCZlANPTa46"},"source":["Define a Function for Splitting Dataset into mini-Batches"]},{"cell_type":"code","execution_count":191,"metadata":{"id":"1_dZER6VTa49"},"outputs":[],"source":["def create_mini_batches(X, y, batch_size=64): # 미니배치를 생성하는 함수를 구현합니다.\n","    mini_batches = [] # 미니배치를 저장할 리스트\n","    n_minibatches = (X.shape[0] // batch_size) # mini-batch의 개수\n","    n_variables = X.shape[1] # feature의 개수\n","    ### START CODE HERE ###\n","\n","    data = np.hstack((X, y))           # concatenate X and y with np.hstack\n","    np.random.shuffle(data)      # then shuffle it\n","    \n","    # create mini-batches\n","    # mini-batch의 개수만큼 반복하면서 mini-batch를 생성합니다.\n","    for i in range(n_minibatches):\n","        mini_batch = data[ (i * batch_size):((i+1) * batch_size)  , :  ]        # get a slice of mini-batch\n","        X_mini, y_mini = mini_batch[:, :n_variables], mini_batch[:, n_variables:]   # split mini-batch into X & y\n","        mini_batches.append((X_mini, y_mini)) # mini-batch를 리스트에 추가합니다.\n","    \n","    # 만약 남은 데이터가 있다면 마지막 mini-batch를 생성합니다.\n","    if data.shape[0] % batch_size != 0:\n","        mini_batch = data[n_minibatches*batch_size : , : ]        # process the remaining data\n","        X_mini, y_mini = mini_batch[:, :n_variables], mini_batch[:, n_variables:]    # split mini-batch into X & y\n","        mini_batches.append((X_mini, y_mini)) # mini-batch를 리스트에 추가합니다.\n","\n","    ### END CODE HERE ###\n","    return mini_batches"]},{"cell_type":"code","execution_count":192,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[[ 4  5]\n"," [18 19]\n"," [12 13]\n"," [ 8  9]]\n","[[-12]\n"," [-19]\n"," [-16]\n"," [-14]] \n","\n","[[ 0  1]\n"," [ 6  7]\n"," [ 2  3]\n"," [14 15]]\n","[[-10]\n"," [-13]\n"," [-11]\n"," [-17]] \n","\n","[[16 17]\n"," [10 11]]\n","[[-18]\n"," [-15]] \n","\n"]}],"source":["np.random.seed(1)  # 랜덤 시드를 설정합니다.\n","\n","a = np.arange(20).reshape(10,2) # 0~19까지의 숫자를 10x2 행렬로 생성합니다.\n","b = -np.arange(10,20).reshape(10,1) # 10~19까지의 숫자를 10x1 행렬로 생성합니다.\n","\n","# 미니 배치를 생성합니다.\n","c = create_mini_batches(a, b, 4)\n","# 미니 배치를 출력하고 확인합니다.\n","for mini_X, mini_y in c: # mini_X와 mini_y를 출력합니다.\n","    print(mini_X)   # mini_X를 출력합니다.\n","    print(mini_y, '\\n') # mini_y를 출력합니다.\n"]},{"cell_type":"markdown","metadata":{},"source":["expected outpu:\n","```\n","[[ 4  5]          [[ 0  1]           [[16 17] \n"," [18 19]           [ 6  7]            [10 11]] \n"," [12 13]           [ 2  3]           [[-18] \n"," [ 8  9]]          [14 15]]           [-15]]  \n","[[-12]            [[-10]             \n"," [-19]             [-13]             \n"," [-16]             [-11]             \n"," [-14]]            [-17]]              \n","```"]},{"cell_type":"markdown","metadata":{},"source":["## Define Various Optimizers\n","\n","Stochastic Gradient $$ g_t = \\nabla J(W_t,x^{(i)},y^{(i)}), \\;\\text{for mini-batch}\\; (i) \\to (i:i+n) $$\n","\n","SGD with momentum $$ \\Delta W(t) = \\gamma \\Delta W (t-1) + \\alpha \\cdot g_t $$\n","AdaGrad $$ \\Delta W(t) = {\\eta {1 \\over \\sqrt{\\delta_t + \\epsilon}}} \\odot g_t, \\;\\text{where}\\; \\delta_t = \\delta_{t-1} + g_t^2 $$\n","RMSProp $$ \\Delta W(t) = {\\eta {1 \\over \\sqrt{\\delta_t + \\epsilon}}} \\odot g_t, \\;\\text{where}\\; \\delta_t = \\beta \\delta_{t-1} + (1-\\beta) g_t^2 $$\n","Adam $$ \\Delta W(t) = {\\eta {\\hat{m}_t \\over \\sqrt{\\hat{v}_t} + \\epsilon}} \\odot g_t, \\;\\text{where}\\; \\hat{m}_t = {m_t \\over {1 - \\beta_1^t}}, \\; \\hat{v}_t = {v_t \\over {1 - \\beta_2^t}}, $$\n","$$ \\text{and}\\; m_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t, \\; v_t = \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2 $$\n","\n","\n","In this experiment mini-batch gradient is used for all optimization methods unless mentioned otherwise.<br>\n","Investigate and discuss the effect on convergence of each optimizer"]},{"cell_type":"code","execution_count":193,"metadata":{},"outputs":[],"source":["class myOptParam:\n","    def __init__(self, n_out, n_in):\n","        # Previoud delta values for momentum optimizer\n","        self.W_dt = np.zeros((n_out, n_in)) # n_out x n_in 크기의 0으로 채워진 행렬을 생성한다. \n","        self.B_dt = np.zeros(n_out) # n_out 크기의 0으로 채워진 행렬을 생성한다.\n","        # Variables for other optimizers \n","        self.W_mt = np.zeros((n_out, n_in)) # n_out x n_in 크기의 0으로 채워진 행렬을 생성한다.\n","        self.B_mt = np.zeros(n_out) # n_out 크기의 0으로 채워진 행렬을 생성한다.\n","        self.W_vt = np.zeros((n_out, n_in)) # n_out x n_in 크기의 0으로 채워진 행렬을 생성한다.\n","        self.B_vt = np.zeros(n_out) # n_out 크기의 0으로 채워진 행렬을 생성한다.\n","\n","# optimizer를 구현합니다.\n","def my_optimizer(lyr, opt, W_grad, B_grad, solver='sgd', learning_rate=0.01, iter=1):\n","    epsilon = 1e-8  # arbitrary small number\n","    alpha = eta = learning_rate # learning rate\n","\n","    if iter==0:\n","        print('iteration should start from 1.')\n","\n","    # optimizer routines\n","    if  solver=='sgd':\n","        W_dlt = alpha * W_grad # delta for weight\n","        B_dlt = alpha * B_grad # delta for bias\n","    elif solver=='momentum':\n","        gamma = 0.9               # default setting\n","        ### START CODE HERE ###\n","        \n","        W_dlt = gamma * opt.W_dt + alpha * W_grad              # momentum for previous delta\n","        B_dlt = gamma * opt.B_dt + alpha * B_grad              # same goes for bias\n","        opt.W_dt = W_dlt           # keep data for later use\n","        opt.B_dt = B_dlt          # for bias, too\n","\n","        ### END CODE HERE ###\n","    elif solver=='adagrad':\n","        ### START CODE HERE ###\n","\n","        opt.W_vt += W_grad**2            # accumulate delta square (2nd momentum)\n","        opt.B_vt += B_grad**2           # accumulater for bias term\n","        W_dlt = (alpha / np.sqrt(opt.W_vt + epsilon)) * W_grad              # calculate new delta for weight\n","        B_dlt = (alpha / np.sqrt(opt.B_vt + epsilon)) * B_grad              # and for bias\n","\n","        ### END CODE HERE ###\n","    elif solver=='rmsprop':\n","        beta2 = 0.9               # default setting\n","        ### START CODE HERE ###\n","\n","        opt.W_vt = beta2 * opt.W_vt + (1-beta2) * (W_grad**2)            # blending with second momentum\n","        opt.B_vt = beta2 * opt.B_vt + (1-beta2) * (B_grad**2)            # also doging samething for bias\n","        W_dlt = alpha / (np.sqrt(opt.W_vt + epsilon)) * W_grad              # calculate new delta for weight\n","        B_dlt = alpha / (np.sqrt(opt.B_vt + epsilon)) * B_grad              # and for bias\n","        \n","        ### END CODE HERE ###\n","    elif solver=='adam':\n","        beta1, beta2 = 0.9, 0.99  # default setting beta1, beta2 = 0.9, 0.99\n","        ### START CODE HERE ###\n","\n","        opt.W_mt = beta1 * opt.W_mt + (1 - beta1) * W_grad          # blending with first momentum\n","        opt.B_mt = beta1 * opt.B_mt + (1 - beta1) * B_grad          # first momentum for bias\n","        opt.W_vt = beta2 * opt.W_vt + (1 - beta2) * (W_grad ** 2)     # blending with second momentum\n","        opt.B_vt = beta2 * opt.B_vt + (1 - beta2) * (B_grad ** 2)     # second momentum for bias\n","\n","        # Bias corrections\n","        W_mc = opt.W_mt / (1 - beta1 ** iter)   # bias correction of first momentum for weight\n","        B_mc = opt.B_mt / (1 - beta1 ** iter)   # and for bias term\n","        W_vc = opt.W_vt / (1 - beta2 ** iter)   # bias correction of second momentum for weight\n","        B_vc = opt.B_vt / (1 - beta2 ** iter)   # and for bias term\n","\n","        W_dlt = alpha * W_mc / (np.sqrt(W_vc) + epsilon)    # calculate new delta for weight\n","        B_dlt = alpha * B_mc / (np.sqrt(B_vc) + epsilon)    # and for bias\n","\n","    \n","        ### END CODE HERE ###\n","    else:  \n","        print('optimizer error')\n","\n","    # Adjust weight\n","    lyr.wegt = lyr.wegt - W_dlt\n","    lyr.bias = lyr.bias - B_dlt\n","\n","    return"]},{"cell_type":"markdown","metadata":{},"source":["Optimizer Test"]},{"cell_type":"code","execution_count":194,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["For sgd:\n","[ 7.67789007  8.16882972 10.34203348 -3.22934657]\n","For momentum:\n","[14.46528172 15.04341688 19.30016537 -4.77070266]\n","For adagrad:\n","[22.50872929 22.74302212 28.47667875 -7.62607443]\n","For rmsprop:\n","[ 30.69802889  30.60433129  37.72651766 -10.62235939]\n","For adam:\n","[29.41774022 19.27573813 23.68071186  1.52919472]\n","test passed.\n"]}],"source":["np.random.seed(101)\n","\n","lyr = myDenseLayer(2,3)\n","opt = myOptParam(2,3)\n","\n","lyr.wegt = np.random.randn(2,3)\n","lyr.bias = np.random.randn(2)\n","opt.W_dt = np.random.randn(2,3)\n","opt.B_dt = np.random.randn(2)\n","opt.W_mt = np.random.randn(2,3)\n","opt.B_mt = np.random.randn(2)\n","opt.W_vt = np.abs(np.random.randn(2,3))\n","opt.B_vt = np.abs(np.random.randn(2))\n","\n","W_grad = np.random.randn(2,3)\n","B_grad = np.random.randn(2)\n","\n","# optimizer settings are: 'sgd', 'momentum', 'adagrad', 'rmsprop', 'adam'\n","opts = ['sgd', 'momentum', 'adagrad', 'rmsprop', 'adam']\n","expt = [[ 7.67789007,  8.16882972, 10.34203348, -3.22934657],\n","        [14.46528172,  15.04341688, 19.30016537, -4.77070266],\n","        [22.50872929,  22.74302212, 28.47667875, -7.62607443],\n","        [30.69802889,  30.60433129, 37.72651766, -10.62235939],\n","        [29.41774022,  19.27573813, 23.68071186,  1.52919472]]\n","test_passed = True # 테스트 결과를 저장할 변수\n","\n","for i, sol in enumerate(opts):  # 5개의 optimizer에 대해 반복합니다.\n","    my_optimizer(lyr, opt, W_grad, B_grad, sol, 10, 3) # optimizer를 실행합니다.\n","    print('For '+sol+':') # optimizer 이름을 출력합니다.\n","    res = np.concatenate((lyr.wegt[0], lyr.bias[0:1]), axis=0) # 가중치와 편향을 합쳐서 출력합니다.\n","    print(res) # 결과를 출력합니다.\n","\n","    if not np.allclose(res, expt[i]):  # 예상 결과와 다르다면\n","        print(sol+' failed.') # 실패 메시지를 출력합니다.\n","        test_passed = False # 테스트 결과를 False로 변경합니다.\n","if test_passed: print('test passed.') # 만약 모든 테스트가 통과했다면 성공 메시지를 출력합니다.\n","else: print('test failed.') # 테스트가 실패했다면 실패 메시지를 출력합니다."]},{"cell_type":"markdown","metadata":{},"source":["**Expected Outputs**\n","\n","For SGD:\n","```\n","[ 7.67789007  8.16882972 10.34203348 -3.22934657]\n","```\n","For Momentum:\n","```\n","[14.46528172 15.04341688 19.30016537 -4.77070266]\n","```\n","Fpr Adagrad:\n","```\n","[22.50872929 22.74302212 28.47667875 -7.62607443]\n","```\n","For RMSProp:\n","```\n","[ 30.69802889  30.60433129  37.72651766 -10.62235939]\n","```\n","For Adam:\n","```\n","[29.41774022 19.27573813 23.68071186  1.52919472]\n","```"]},{"cell_type":"markdown","metadata":{},"source":["Create Optimizer Parameters"]},{"cell_type":"code","execution_count":195,"metadata":{},"outputs":[],"source":["o1 = myOptParam(n_hidden1, n_inputs) # 첫번째 은닉층의 optimizer\n","o2 = myOptParam(n_hidden2, n_hidden1) # 두번째 은닉층의 optimizer\n","o3 = myOptParam(n_classes, n_hidden2) # 출력층의 optimizer"]},{"cell_type":"markdown","metadata":{},"source":["Training Simple Neural Network Model (3 layer model)"]},{"cell_type":"code","execution_count":196,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":55777,"status":"ok","timestamp":1649259680196,"user":{"displayName":"Seong-Won Lee","userId":"14858304546124675216"},"user_tz":-540},"id":"qODinrZlTa5C","outputId":"8237949a-964a-49cd-f3e9-fb65759245e7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch:  100,  loss: 0.00498634\n","Epoch:  200,  loss: 0.00224317\n","Epoch:  300,  loss: 0.00142530\n","Epoch:  400,  loss: 0.00103584\n","Epoch:  500,  loss: 0.00080901\n","Epoch:  600,  loss: 0.00066159\n","Epoch:  700,  loss: 0.00055858\n","Epoch:  800,  loss: 0.00048267\n","Epoch:  900,  loss: 0.00042447\n","Epoch: 1000,  loss: 0.00037846\n"]}],"source":["# optimizer settings are: 'sgd', 'momentum', 'adagrad', 'rmsprop', 'adam'\n","# alpha is learning rate\n","optimizer ='sgd' # optimizer를 설정합니다.\n","alpha = 0.01 # learning rate를 설정합니다.\n","n_epochs = 1000 # epoch 수를 설정합니다.\n","\n","for epoch in range(n_epochs):\n","    # create mini-batches\n","    batches = create_mini_batches(X_train, y_train, batch_size=64)\n","    # mini-batch를 반복하면서 학습을 진행합니다.\n","    for one_batch in batches:\n","        # mini-batch에서 X와 y를 가져옵니다.\n","        X_mini, y_mini = one_batch\n","        batch_len = X_mini.shape[0]  # last batch might have different length\n","\n","        # Forward Path\n","        a_1, a_2, a_3 = my_forward(layers, X_mini)\n","        \n","        # Backward Path\n","        d_1, d_2, d_3 = my_backward(layers, a_1, a_2, a_3, X_mini, y_mini)\n","\n","        dw_1, db_1 = d_1\n","        dw_2, db_2 = d_2\n","        dw_3, db_3 = d_3\n","        \n","        # Update weights and biases\n","        my_optimizer(l1, o1, dw_1, db_1, solver=optimizer, learning_rate=alpha, iter=epoch+1) # optimizer를 실행합니다. \n","        my_optimizer(l2, o2, dw_2, db_2, solver=optimizer, learning_rate=alpha, iter=epoch+1) # optimizer를 실행합니다.\n","        my_optimizer(l3, o3, dw_3, db_3, solver=optimizer, learning_rate=alpha, iter=epoch+1) # optimizer를 실행합니다.\n","\n","    # 100번째 epoch마다 loss를 출력합니다.\n","    if ((epoch+1)%100==0): \n","        loss_J = my_loss(layers, X_train, y_train) # loss를 계산합니다.\n","        print('Epoch: %4d,  loss: %10.8f' % (epoch+1, loss_J)) # epoch와 loss를 출력합니다."]},{"cell_type":"markdown","metadata":{},"source":["Evaluate Model Performance"]},{"cell_type":"code","execution_count":197,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":292,"status":"ok","timestamp":1649259686353,"user":{"displayName":"Seong-Won Lee","userId":"14858304546124675216"},"user_tz":-540},"id":"xMvLn6SJTa5D","outputId":"229cafc3-c9c5-4dd2-f7bc-d4cdf90c4d46"},"outputs":[{"data":{"text/plain":["0.9527777777777777"]},"execution_count":197,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.metrics import accuracy_score\n","\n","# 학습 데이터에 대한 예측을 수행합니다.\n","y_pred = my_predict(layers, X_test)\n","\n","# 정확도를 계산합니다.\n","accuracy_score(y_pred, y_test)"]},{"cell_type":"markdown","metadata":{"id":"WieUxPz9Ta5F"},"source":["Neural Network from scikit-learn"]},{"cell_type":"code","execution_count":198,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3086,"status":"ok","timestamp":1649259694763,"user":{"displayName":"Seong-Won Lee","userId":"14858304546124675216"},"user_tz":-540},"id":"w8AcitiaTa5G","outputId":"09455c7b-a58f-4492-b5cb-431eab436f6c"},"outputs":[{"data":{"text/plain":["0.9777777777777777"]},"execution_count":198,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.neural_network import MLPClassifier  # 다층 퍼셉트론(MLP)을 구현하는 클래스\n","\n","mlp = MLPClassifier(\n","    hidden_layer_sizes=(80, 70, ),  # 은닉층의 크기를 정의. 여기서는 2개의 은닉층을 사용하며, 첫 번째 은닉층에는 80개의 뉴런, 두 번째 은닉층에는 70개의 뉴런을 사용\n","    activation='logistic',          # 활성화 함수로 'logistic' 함수를 사용 (로지스틱 시그모이드 함수).\n","    solver='sgd',                   # 최적화 알고리즘으로 확률적 경사 하강법(SGD)을 사용.\n","    alpha=0.01,                     # 학습률에 곱해지는 상수. 학습률을 조절하는 하이퍼파라미터\n","    learning_rate_init=0.01,        # 경사 하강법에서 가중치 업데이트의 크기를 결정\n","    max_iter=1000,                   # 최대 반복 횟수. 모델 학습 시 최대 1000번의 반복까지 수행. 이 횟수에 도달하면 학습이 멈춤\n",")\n","# Training/Fitting the Model\n","mlp.fit(X_train, y_train_num) # 학습 데이터에 대해 모델 학습\n","\n","# Making Predictions\n","s_pred = mlp.predict(X_test) # 테스트 데이터에 대한 예측값 출력\n","accuracy_score(s_pred, y_test) # 정확도 출력"]},{"cell_type":"markdown","metadata":{"id":"BmZQrDH9n0PK"},"source":["### Test Model with a random sample\n"]},{"cell_type":"code","execution_count":199,"metadata":{},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 200x200 with 1 Axes>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["My prediction is 3\n","sk prediction is 3\n","Actual number is 3\n"]}],"source":["idx = np.random.randint(X_test.shape[0]) # 0부터 X_test의 크기까지 랜덤한 정수를 생성합니다.\n","dimage = X_test_org[idx].reshape((8,8)) # X_test_org의 idx번째 데이터를 8x8로 변환합니다.\n","plt.figure(figsize=(2,2)) # 그래프의 크기를 설정합니다.\n","plt.gray() # 그래프의 색상을 흑백으로 설정합니다.\n","plt.matshow(dimage, fignum=1) # 이미지를 출력합니다.\n","plt.show() # 그래프를 출력합니다.\n","\n","X_input = np.expand_dims(X_test[idx], 0) # X_test의 idx번째 데이터를 2차원으로 변환합니다.\n","\n","y_pred = my_predict(layers, X_input) # 내 모델을 사용하여 예측합니다.\n","\n","s_pred = mlp.predict(X_input) # sklean 모델을 사용하여 예측합니다.\n","\n","print('My prediction is ' + str(y_pred[0])) # 내 모델의 예측을 출력합니다.\n","print('sk prediction is ' + str(s_pred[0])) # sklearn 모델의 예측을 출력합니다.\n","print('Actual number is ' + str(y_test[idx])) # 실제 정답을 출력합니다.\n"]},{"cell_type":"markdown","metadata":{},"source":["(c) 2024 SW Lee"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"ML_L06_01_MNIST.ipynb","provenance":[]},"kernelspec":{"display_name":"Lee_professor","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.15"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
